---
title: "HW6: Model Selection & Multicollinearity (mostly ALRM Ch. 9, 10)"
author: "MATH/STAT 571A"
date: "**DUE: 11/17/2023 11:59pm**"
header-includes:
   - \renewcommand*\familydefault{\sfdefault} %% this picks a sans serif font
   - \usepackage[T1]{fontenc}
output: pdf_document
---

```{r setup, echo=F}
knitr::opts_chunk$set(cache = T, fig.width = 4, fig.align = 'center', echo = F)
lobsters <- read.csv('./Datasets/lobsters_survey.csv')
```

## Homework Guidelines

***Please submit your answers on Gradescope as a PDF with pages matched to question answers.***

One way to prepare your solutions to this homework is with R Markdown, which provides a way to include mathematical notation, text, code, and figures in a single document. A template `.Rmd` file is available through D2L. 

Make sure all solutions are clearly labeled, and please utilize the question pairing tool on Gradescope. You are encouraged to work together, but your solutions, code, plots, and wording should always be your own. Come and see me and/or our TA during office hours or schedule an appointment when you get stuck and can't get unstuck.

### I. Mathematical Foundations [14 pts]

The goal of the next two questions is to derive ALRM (10.21), $d_i = \frac{e_i}{1 - h_{ii}}$, for the special case of simple linear regression through the origin (i.e., $Y_i = \beta X_i + \varepsilon_i$).

Let $X$ and $Y$ be the vectors $(X_1, \dots, X_n)$ and $(Y_1, \dots, Y_n)$, respectively, and let $X_{(i)}$ and $Y_{(i)}$ be the sub-vectors without the $i$th entry, such that $X_{(i)} = (X_1, \dots, X_{i - 1}, X_{i + 1}, \dots, X_n)$ and $Y_{(i)}$ defined analogously. Finally, let $b_{(i)} = (X_{(i)}'X_{(i)})^{-1}X_{(i)}'Y_{(i)}$ be the least-squares estimate of $\beta$ based on $X_{(i)}$ and $Y_{(i)}$ and let $\hat{Y}_{i(i)} = X_i b_{(i)}$ denote the associated predicted value for the response at predictor level $X_i$.

(1) [3 pts] Show that for $h_{ii} = X_i (X'X)^{-1}X_i$, $X_{(i)}'X_{(i)} = X'X(1 - h_{ii})$ (*hint: note that $X^\prime X = X_{(i)}'X_{(i)} + X_i^2$*).

(2) [5 pts] Use the result from (1) to show that $Y_i - \hat{Y}_{i(i)} = d_i = \frac{e_i}{1 - h_{ii}}$.

The goal of the next question is to verify the formula for deleted residuals numerically for a model of spiny lobster density.

(3) [6 pts] Fit a linear regression model for the cube-root of lobster density using the predictors `"MPA", "Depth_m", "Relief_cm", "Flat_Rock", "Cobble", "Boulder", "Sand"` and the interaction between `"Depth_m"` and `"Flat_Rock"` (see question 5 from HW5), and verify the first residual value is 0.02743954. Next, fit the same model, but without the response or predictors for observation 1. Use the fitted model object to predict the cube-root of lobster density based on the predictor values associated with the 1st observation:

0.275 is the predicted.
```{r, echo = T}
predictors_rock <- c("MPA", "Depth_m", "Relief_cm", 
                     "Flat_Rock", "Cobble", "Boulder", "Sand")
lobsters[1, predictors_rock]

original_formula <- Lob_dens^(1/3) ~ MPA + Relief_cm + 
                         Depth_m * Flat_Rock + Cobble + Boulder + Sand
lobsters_cubefit <- lm(original_formula, data = lobsters)
lobsters_cubefit$residuals[1] # matches

lobsters_cubefit2 <- lm(original_formula, data = lobsters[-1,])
pred_val <- predict(lobsters_cubefit2, newdata = lobsters[1, predictors_rock])
pred_val
# 0.275
real_val <- lobsters[1,"Lob_dens"]^(1/3)
real_val
# 0.307 is the actual
```
  > **Report the predicted value, $\hat{Y}_{1(1)}$.** Confirm that the deleted residual obtained through re-fitting the model is the same as the one obtained using the formula $d_i = \frac{e_i}{1 - h_{ii}}$.
  
```{r, echo = T}
X <- model.matrix(lobsters_cubefit)
H <- X %*% solve(t(X) %*% X) %*% t(X)
d_1 <- lobsters_cubefit$residuals[1]/(1-H[1,1])

c(real_val - pred_val, d_1)
```

### II. Lobsters in Southern California [26 pts]

We will be using recently gathered data about the abundance of the California spiny lobster (*Panulirus interruptus*) along the southern coast of California. The California spiny lobster is an important commercial species for California, and there was concern that over-fishing might lead to a decline in the population of the species. In 2012, several marine protected areas (MPAs) were established where fishing is prohibited. The MPAs were not established explicitly to protect spiny lobsters, but they offer a natural experiment where abundance of spiny lobsters inside and outside the MPAs can be studied to see what effect the protected areas might have.

In 2012 and 2013, a team of researchers collected counts of spiny lobsters around 5 different MPAs along the coast near San Diego. Counts were made just inside and just outside each MPA. This [**website**](https://data.ca.gov/dataset/california-spiny-lobster-panulirus-interruptus-california-south-coast-mpa-baseline-study-2-2013/resource/248e2442-4c34-49cb-b6e2-aa4391db8a1d) gives information about all the variables measured at each site, and this [**website**](https://data.ca.gov/dataset/california-spiny-lobster-panulirus-interruptus-california-south-coast-mpa-baseline-study-2-2013/resource/6dd93320-c56d-4cf9-8a95-999fa2fe1f9c) gives the raw data (also posted on D2L). Your goal is to use linear models to study the relationships between the density of lobsters observed at each site and the characteristics of the sites.

(4) [3 pts] Make a plot of Cook's distances for each observation from the model fit in (3) with all observations. With which MPA is the observation with the largest Cook's distance associated? Is anything special/interesting about this observation? Do you see any reason why it should be removed from the analysis?

Observation 54 has MPA "Laguna Beach State Marine Reserve" associated with it. It has an abnormally tall relief structure (524 cm compared to 2nd tallest relief: 83.6 cm). Because the abnormally extreme relief height, lobster density may be affected differently by this one. Therefore, it seems reasonable to remove from this dataset.
```{r, echo = T}
plot(lobsters_cubefit, 4)

lobsters[54, predictors_rock]
head(sort(lobsters$Relief_cm, decreasing = T))
```
(5) [4 pts] Compute DFFITS for the model fit in (3). Which observation has the value with the largest magnitude? Compute DFBETAS for each regression coefficient. Which combination of observation and predictor variable has the value with the largest magnitude?

The observation with the largest magnitude DFFITS value is Obs. 54 with DFFITS: -3.309129.
DFBETAS: Observation 54 with Relief_cm variable.
```{r, echo = T}
# DFFITS
n <- nrow(lobsters)
p <- length(lobsters_cubefit$coefficients)
SSE <- sum(lobsters_cubefit$residuals^2)

e <- rep(0,n)
t <- rep(0,n)
DFFITS <- rep(0,n)

for (i in 1:n) {
  e[i] <- lobsters_cubefit$residuals[i]
  t[i] <- e[i] * ( (n-p-1) / ( SSE*(1-H[i,i]) - e[i]^2 ) )^(1/2)
  DFFITS[i] <- t[i] * ( (H[i,i]) / (1 - H[i,i]) )^(1/2)
}

largestDFFITS <- sort(DFFITS, decreasing = F)[1]
match(largestDFFITS,DFFITS)
# 54
lobsters[54, predictors_rock]
```
```{r, echo = T}
library(dplyr)
# DFBETAS
DFBETAS <- as.data.frame(matrix(0, nrow=n, ncol=p, byrow=T))
parameters <- names(lobsters_cubefit$coefficients)
names(DFBETAS) <- parameters
rownames(DFBETAS) <- 1:n 

for_c <- solve(t(X)%*%X)
for (i in 1:n){
  diff_fit <- lm(original_formula, data = lobsters[-c(i),])
  MSE_diff <- mean(diff_fit$residuals^2)
  
  for (k in 1:p) {
    b_k <- lobsters_cubefit$coefficients[[k]]
    b_k_diff <- diff_fit$coefficients[[k]]
    c_kk <- for_c[k,k]
    DFBETAS[i,k] <- (b_k - b_k_diff) / sqrt(MSE_diff * c_kk)
  }
}

big_beta <- max(-DFBETAS)
DFBETAS <- data.frame(DFBETAS, removed_obs = 1:n)
DFBETAS %>% filter(if_any(.cols = everything(),
                          .fns = ~ .x == -big_beta)
                   )
# Obs 116 and Cobble
```
(6) [3 pts] Compute the sample correlations for all pairs of the variables `"Depth_m", "Relief_cm", "Flat_Rock", "Cobble", "Boulder", "Sand"`. Do you see any evidence of multicollinearity? If so, which variables appear to be involved?

Yes, we see multicollinearity within two pairs of predictors:
1) Flat Rock and Boulder
2) Flat Rock and Sand

```{r, echo = T}
library(corrplot)
X <- model.matrix(lobsters_cubefit, data = lobsters)[,c(-1,-12)]
lobsters_cubefit$coefficients
corrplot(cor(X), type = "lower", tl.cex = 0.65)
```

(7) [3 pts] Compute generalized variance inflation factors for all variables in the model (3). Do you see any evidence of multicollinearity? If so, which variables appear to be involved?

Flat_Rock and Depth_m appear to be involved.
```{r, echo = T}
library(car)
vif(lobsters_cubefit, type = 'predictor')
```

(8) [3 pts] Begin with the model from (3). Use the `step()` function in R to select a model using backwards step selection based on AIC. Which variables are removed compared the initial model?

Relief_cm was removed.

Backwards model: MPA, Depth_m, Flat_Rock, Cobble, Boulder, Sand, Depth_m:Flat_Rock
```{r, echo = T}
bw_step <- step(lobsters_cubefit, k = 2)
original_var <- names(lobsters_cubefit$coefficients)
bw_var <- names(bw_step$coefficients)

original_var[!(original_var %in% bw_var)]
```

(9) [3 pts] Use the `step()` function again to select a model, but this time start with a model that only includes the variables `"MPA", "Depth_m", "Relief_cm"` (still with a cube-root transformed response) and take steps in both directions. Were any variables included in the best model using backward step selection, but not included when taking steps in both directions? If so, which ones?

Step-wise in both direction led to less variables. The "Cobble" and "Sand" variables were eliminated.
Backwards model: MPA, Depth_m, Flat_Rock, Cobble, Boulder, Sand, Depth_m:Flat_Rock
Both model: MPA, Depth_m, Flat_Rock, Boulder, Depth_m:Flat_Rock
```{r, echo = T}
both_step <- step(lm(Lob_dens^(1/3) ~ MPA + Depth_m + Relief_cm, data = lobsters), k = 2, 
     scope = list(upper = original_formula, 
                  lower = Lob_dens^(1/3) ~ 1, direction = 'both'))

both_var <- names(both_step$coefficients)

both_var
bw_var

both_var[!(both_var %in% bw_var)]
bw_var[!(bw_var %in% both_var)]
```
(10) [4 pts] Use the `glmnet` package to implement cross validation to estimate the mean-squared error across a range of values for the penalty term, $\lambda$, for a LASSO penalty. Determine and report a value for $\lambda$ that seems optimal to you. Explain how you arrived at your value.

I chose 0.01684 as the lambda since it is 1 se above the minimum. Choosing this value will result in a more parsimonious model (less terms).
```{r, echo = T, fig.height = 4, fig.width = 5}
library(glmnet)
X <- model.matrix(original_formula, data = lobsters)
cvfit <- cv.glmnet(x = X, y = lobsters$Lob_dens^(1/3), alpha = 1)
print(cvfit)
```

(11) [3 pts] Fit the LASSO-penalized linear regression model with variables from (3) and your value of $\lambda$ from the previous problem. Which variables have non-zero coefficients? How does your fit compare to the model selected using backwards stepwise selection in (8)?

LASSO removed Flat_Rock, Cobble, and Depth_m:Flat_Rock interaction term.
LASSO model: MPA, Depth_m, Boulder, and Sand.
Backwards model: MPA, Depth_m, Flat_Rock, Cobble, Boulder, Sand, Depth_m:Flat_Rock

```{r, echo = T, fig.height=4, fig.width=5}
coef(glmnet(X, lobsters$Lob_dens^(1/3)), s = 0.02443)
```
