---
title: "HW2: SLR Inference and Diagnostics (ALRM Ch. 2, 3)"
author: "MATH/STAT 571A"
date: "**DUE: 09/22/2023 11:59pm**"
header-includes:
   - \renewcommand*\familydefault{\sfdefault} %% this picks a sans serif font
   - \usepackage[T1]{fontenc}
output: pdf_document
---

## Homework Guidelines

***Please submit your answers on Gradescope as a PDF with pages matched to question answers.***

One way to prepare your solutions to this homework is with R Markdown, which provides a way to include mathematical notation, text, code, and figures in a single document. A template `.Rmd` file is available through D2L. 

Make sure all solutions are clearly labeled, and please utilize the question pairing tool on Gradescope. You are encouraged to work together, but your solutions, code, plots, and wording should always be your own. Come and see me and/or our TA during office hours or schedule an appointment when you get stuck and can't get unstuck.

### I. Mathematical Foundations [12 pts]
(@) [5 pts] Let $Y_1, \dots, Y_n$ be a sample of response values associated with predictor values $X_1, \dots, X_n$. Suppose that $\tilde{Y} = b_0 + b_1 X$, $b_1 \neq 0$, is a line that passes through $(\bar{X}, \bar{Y})$ and satisfies the sums of squares relationship $\sum (Y_i - \bar{Y})^2 = \sum (\tilde{Y}_i - \bar{Y})^2 + \sum (Y_i - \tilde{Y}_i)^2$. Show that $b_0$ and $b_1$ must be the least-squares estimates of the simple linear regression of $Y_i$ onto $X_i$ (hint: lines that pass through $(\bar{X}, \bar{Y})$ must satisfy $\tilde{Y}_i - \bar{Y} = b_1(X_i - \bar{X})$).

(@) [3 pts] ALARM 2.18: For conducting statistical tests concerning the parameter $\beta_1$, why is the $t$ test more versatile than the $F$ test?

The T-test is more versatile because it can be used for one-sided alternative hypotheses which the F-test cannot.

(@) [4 pts] ALRM 3.2: Prepare a prototype residual plot for each of the following cases. You can prepare your plots in R, or draw them neatly by hand. 

    (i) error variance decreases with $X$
```{r}
x = seq(1,100,by=.5)
b0 = 0
b1 = 1
sigma2 = x^-1
eps = rnorm(x, mean = 0, sd = sqrt(sigma2))
y = b0 + b1*x + eps
fit = lm(y ~ x)
plot(fit,1)
```
    (ii) the true regression function is "U"-shaped, but a linear regression function is fitted. 
```{r}
x = seq(1,100,by=.5)
b0 = 0
b1 = 1
sigma = 10
eps = rnorm(x, mean = 0, sd = sigma)
y = b0 + b1*x^1.2 + eps
fit = lm(y ~ x)
plot(fit,1)
```

### II. Tree Cover in Tucson [28 pts]

Download the Tree Equity Scores data set from Module 1 on D2L (`Tree_Equity_Scores_Tucson_noNA.csv`). Use the `read.csv()` function to import the comma spaced values as a dataframe in R. There are several variables recorded in this data set, but we will continue to focus primarily on two (read more here: https://gisdata.tucsonaz.gov/datasets/cotgis::tree-equity-scores-tucson-1/about). The first is `PCTTreeCover`, the percent of each corresponding census tract that is covered by tree canopy. The second is `PCTPoverty`, the percentage of households in each census tract with an income below the poverty line. We will also look at the percentage of residents in each tract that are children (under 17), `PCTChildren`, for one question.

Treat percent tree cover as the response variable and percent below poverty line as a predictor. 

(@) [2 pts] Obtain a 99% confidence interval for $\beta_1$, the effect of percent below poverty line on tree cover. Does it include 0?

The 99% confidence interval for beta_1 is [-0.07013332, -0.006111215] and does not contain 0.

```{r}
trees <- read.csv('./Datasets/Tree_Equity_Scores_Tucson_noNA.csv')
fit_trees <- lm(PCTTreeCover ~ PCTPoverty, data = trees)
confint(object = fit_trees, level = 0.99)
```

(@) [2 pts] Report the p-value for the two sided $t$-test for whether or not a linear relationship exists between the predictor and response. Do you reject the null hypothesis of no linear relationship at the $\alpha = 0.01$ level? How does your conclusion reconcile with your confidence interval from the previous question?

The p-value is 0.0022 which allows us to reject the null hypothesis at a 0.01 alpha level. This agrees with our 99% confidence which does not include 0.
```{r}
summary(fit_trees)
```

(@) [3 pts] Compute and report the standard error, $s(\hat{Y}_h)$, for a prediction of the expected percent tree cover in census tracts where 20% of residents have incomes below the poverty line. Obtain a 90% confidence interval for the expected percent tree cover in census tracts with 20% of residents below the poverty line. What is the width of your interval?

The width is 1.142882.

```{r}
pred_interval = predict(fit_trees, newdata = data.frame(PCTPoverty = 20), 
                        interval = 'confidence',
                        level = .9)
pred_interval
```

```{r}
pred_interval[[3]]-pred_interval[[2]]
```

(@) [3 pts] Compute and report the standard error, $s(\mathrm{pred})$, for a prediction of percent tree cover in a new census tract where 20% of residents have incomes below the poverty line. Obtain a 90% confidence interval for the prediction at a new tract. What is the width of your interval?

The width is 15.77207.

```{r}
pred_interval = predict(fit_trees, newdata = data.frame(PCTPoverty = 20), 
                        interval = 'predict',
                        level = .9)
pred_interval

```

```{r}
pred_interval[[3]]-pred_interval[[2]]

```

(@) [3 pts] Determine the boundary values of the 90% Working-Hotelling confidence band for the regression line when the percent of residents with an income below the poverty line is 20%. What is the width of your interval?

The width is 1.605366.

```{r}
# Taken from Session 4
pred_WH <- function(object, newdata, level = 0.95){
  fit <- predict(object, newdata)
  MSE <- summary(object)$sigma^2
  n <- nrow(object$model) 
  W <- qf(level, 2, n - 2)
  
  X_obs <- object$model[, attr(object$terms, "term.labels")]
  X <- newdata[, attr(object$terms, "term.labels")]
  ME <- W * sqrt(MSE * (1 / n + (X - mean(X_obs))^2 / 
                          sum((X_obs - mean(X_obs))^2)))
  upr <- fit + ME
  lwr <- fit - ME
  return(cbind(fit, lwr, upr))
}
WH_pred <- pred_WH(fit_trees, newdata = data.frame(PCTPoverty = 20), level = 0.9)
WH_pred
```
```{r}
WH_pred[[3]] - WH_pred[[2]]
```

(@) [2 pts] Prepare a histogram of the predictor values. Do you notice any potential outliers?

There appear to be 7 outliers at the 0 value for PCTPoverty.

```{r}
x = trees$PCTPoverty
head(sort(x), n = 10)
```
```{r}
hist(x, breaks = 100)
```
(@) [3 pts] Prepare plots of the residuals vs. both the fitted values and predictor values. What potential issue(s) with the normal SLR model does your two diagnostic plots raise? Describe what concerning feature(s) you see in the figures.

We see heteroskedasticity:
-lower predictor values correspond to higher variance in predictions
-higher fitted values have higher variance in predictions
Overall, this indicates a negative relationship that has decreasing variation with larger predictor values.

```{r, fig.width = 3, fig.height = 3}
# Residuals v. predictor
plot(trees$PCTPoverty, fit_trees$residuals, ylab = "residuals", xlab = "Fitted values")
# Residuals v. fitted
plot(fit_trees,1)
```
```{r, fig.height = 4, fig.width = 5}
plot(trees$PCTPoverty, trees$PCTTreeCover, main = "PCTPoverty v PCTTreeCover",
     xlab = "PCTPoverty", ylab = "PCTTreeCover")
```
(@) [2 pts] Prepare a QQ-plot comparing the residuals to a normal reference distribution. What potential issue(s) with the normal SLR model does your two diagnostic plots raise? Describe what concerning feature(s) you see in the figure.

The QQ plot reveals that there is higher variation at the larger fitted values which can be inferred from the visibly fatter tail on the right.

```{r, fig.height = 4, fig.width = 5}
plot(fit_trees,2)
```

(@) [3 pts] Make a plot of the residuals on the vertical axis and the percentage of residents who are children (under 17). Does your plot suggest that this might be an important omitted variable for predicting percent tree cover? Why/why not?

It appears that PCTChildren is an important variable that is related to the model. The variation decreases with increasing values of PCTChildren. 

```{r, fig.height = 4, fig.width = 5}
plot(x = trees$PCTChildren, y = fit_trees$residuals)

```

(@) [2 pts] Suggest a transformation to either the predictor or response variable that might address one of the concerns you raised in (10). Be sure to indicate what issue the transformation is intended to address and why you think it might help.

I would like to use the boxcox transformation to address the high variance at lower-valued predictor values.


(@) [3 pts] Implement the transformation you suggested in (13) and comment on how much it helped.

It has helped reduce the changing variance drastically.

```{r, fig.height = 3, fig.height = 3}
library(MASS)
bc <- boxcox(fit_trees)
exp <- bc$x[which.max(bc$y)]

fit_bc <- lm(PCTTreeCover^exp ~ PCTPoverty, data = trees)
plot(fit_bc, c(1,2))
```

(*BONUS*) [3 pts] Conduct a test for lack of fit by binning predictor values into bins of width 5 and performing an appropriate general linear test. Is there sufficient evidence to reject the linear model at the $\alpha = 0.01$ level?